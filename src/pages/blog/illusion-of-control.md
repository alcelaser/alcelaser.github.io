---
layout: ../../layouts/ArticleLayout.astro
title: "The Illusion of Control: Why AI No-Code Tools May Not Empower Users"
description: "AI-driven no-code platforms promise to democratize software creation, giving non-technical users the power to build tools once reserved for specialists. The appeal is obvious: speed, flexibility, and accessibility..."
publication: "Google Developer Group Guido Carli"
originalUrl: "https://googleguidocarli.wordpress.com/2025/10/01/the-illusion-of-control-why-ai-no-code-tools-may-not-empower-users/"
---

AI-driven no-code platforms promise to democratize software creation, giving non-technical users the power to build tools once reserved for specialists. The appeal is obvious: speed, flexibility, and accessibility; but beneath the surface lie risks of unreliability, security gaps, and growing dependence on opaque systems. Is this genuine empowerment, or an illusion of control? Perhaps the real value of AI is not replacing developers entirely, but reshaping how experts and citizen creators collaborate.

---

Since the beginning of programming, developers have strived to make the process of creating software easier. Whether that be from the creation of the first "High Level" programming languages like FORTRAN and COBOL, made to make programming accessible to engineers and financiers respectively, to the creation of foundational languages like C, skipping the need to use Assembly entirely, developers are always trying to make programming easier, and not just for themselves. In the age of AI, we find more and more non-technical users or citizen developers interacting with technology they haven't seen before and quickly finding their limits. These citizen developers could be anyone, from teachers to small business owners, united by the fact that, despite not being able to program the tools they need, their digital needs are unmet.

This might not be the case anymore.

Since 2021, we've seen a surge in no-code platforms, tools that create customised software without the traditional barriers of technical complexity, usually with drag and drop elements. This already opens the doors to many people who wouldn't have otherwise attempted to create a solution by themselves. But they aren't perfect yet, and nowhere nearly as user-friendly as they sound. The complexity of the tool created is limited by what the platform can do, and at this time, code produced by no-code platforms still requires oversight from a dedicated IT Team.

According to Forbes, most new applications are created using low-code or no-code solutions. By integrating AI into no-code tools, citizen developers have access to even more customisation, but unlike traditional no-code tools that use logical building blocks to create software, AI uses natural language, which is far more user-friendly, although its inner workings are more akin to a black-box. Thus, can AI truly empower non-technical users? Or will it leave them dependent on black-box tools they cannot control? And more importantly, should society even strive for such a future?

When it comes to how feasible these platforms are in practice, we already have data to back up their utility. Tools like LowCoder show that when AI is integrated, users perform better: in one study, participants discovered relevant AI operators 75% of the time, compared to just 32.5% with traditional search. In Industrial Internet-of-Things contexts, SeLoC-ML reduced engineering effort by a factor of three or more, enabling non-specialists to create machine learning applications without expert oversight. These all seem like great successes, but in practice, building a pipeline for IoT or automating a small UI workflow is not the same as creating a fully integrated business application. As analysts note, while AI can accelerate automation development, it still struggles with reliability, contextual understanding, and long-term usability.

AI enthusiasts would argue that tools like ML-Quadrat already enable "citizen data scientists" to construct machine learning workflows with diagrammatic and form-based interfaces, and the benefits are clearly tempting: the idea of a workspace where all members of an organisation can contribute equally to a project, with faster deployment and greater flexibility. But while this all sounds innovative and empowering, the trade-offs just might outweigh the benefits for now.

The truth is that AI tools do not guarantee correctness. Since LLMS are probabilistic models, they can misinterpret user intent relatively often, and tend to produce subtly flawed code, so subtle that even an experienced developer may struggle to debug the code. Unfortunately, citizen developers do not have the required skills to effectively fix these hidden bugs. More importantly, real-world value depends not just on stand-alone tools but on connecting them to existing infrastructure, databases, APIs, and legacy systems. If the AI toolâ€™s context window isn't large enough, it won't be able to account for them all, rendering it effectively of no value.

Legality is also a great issue in this context. Allowing non-technical staff to build their own tools introduces what organizations call shadow IT: applications created outside formal IT processes. These can compromise security, regulatory compliance, and organizational consistency. When it comes to responsibility, is it up to the prompter?

The bigger concern may be philosophical. If software creation becomes so abstracted that users no longer understand how it works, does this represent empowerment or dependency? Over-reliance on opaque AI platforms risks eroding problem-solving skills while locking users into proprietary ecosystems they cannot escape. This "illusion of control" could leave individuals and organizations at the mercy of a handful of technology providers. With this in mind, perhaps the better question isn't if we can, but if we should. True technological empowerment should be achieved by balancing all of the factors: governance, oversight, ethical, legal, and structural integrity. By using AI as a bridge to communicate between software developers and citizen developers, great strides can be achieved. Custom software still matters because it ensures maintainability and transparency in ways no-code platforms cannot.

To conclude, despite being relatively early in their development, AI-powered no-code platforms offer a tantalizing glimpse of a world where non-technical users can build the tools they need. The promise is clear: faster development, greater flexibility, and a democratization of software creation that has long been confined to specialists. Yet the reality is more complicated. Reliability, integration challenges, legal accountability, and the risk of dependency on opaque systems remain serious obstacles. AI can be a bridge between citizen developers and professional software engineers, enabling collaboration and innovation, but it cannot replace the rigor, transparency, and maintainability that custom software provides. In the end, the question is not just whether we can empower non-technical users with AI, but whether we should, and how to do so responsibly.
