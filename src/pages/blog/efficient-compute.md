---
layout: ../../layouts/ArticleLayout.astro
title: "The Efficient Compute Frontier – AI's Great Filter"
description: "Discusses efficiency in AI development and its potential constraints on future innovation."
publication: "Google Developer Group Guido Carli"
originalUrl: "https://googleguidocarli.wordpress.com/2025/04/15/the-efficient-compute-frontier-ais-great-filter/"
---

Following OpenAI’s breakthrough in Large Language Models, the Machine Learning world has expanded massively in an incredibly short time, and it’s not looking like it’s going to stop anytime soon, but hypothetically, how soon would it be until we reach the limit? In this article, we’ll explore together how fundamentally these systems ‘learn’ and where any potential pitfalls may lie, in order to try and predict how the future of these machine learning models will grow, akin to how Machine Learning models actually function.
Transformers like OpenAI’s GPT-4o (Generative Pretrained Transformer) are the most popular at the moment, and the secret to their success is not that surprising. Unlike past breakthroughs in Machine Learning, these new massive Transformer models are much less specialised for a specific task as they are prepared to tackle what might seem as almost everything. They make Machine Learning accessible, but the underlying principles are nothing new, a Transformer essentially is a massive complex distribution function, created to learn patterns from data, and weighing the probability of a good output on a set of predetermined ‘weights’, which are acquired by testing the model repeatedly on data for which we already know the correct answer to see how often it makes mistakes and adjusting accordingly. Typically, the models improve in a characteristic way, the cleaner the data and the more data we have, the better the model. This is true in the beginning, initially errors drop off quickly, the rate of improvement grows but it slows down as time goes on, and keeps slowing down, eventually leveling off.
If we invest more computational resources (‘compute’) to train a larger model (one with more internal ‘learnable parameters’), it typically achieves a lower final error rate, but it follows a similar curve. Repeating this process with progressively larger models and more compute yields a family of performance curves. A striking pattern emerges when we plot these curves using logarithmic scales for both performance (error/loss) and the resources invested (compute, model size, or dataset size): the performance improvements often trace straight lines. Critically, these lines suggest a boundary – a performance level that, for a given amount of compute, models seem unable to cross. This boundary is often referred to as the efficient compute frontier.
This frontier represents the best performance achievable for a given computational budget. This observed trend is one aspect of neural scaling laws: error rate appears to scale predictably, often as a power law, with compute, model size, and dataset size. Surprisingly, studies have shown that this trend is true regardless of model architecture or algorithmic choices, provided rational choices are made on the model. Thus, this begs the question, is there a principle governing how much we can ‘push’ these models?
Let’s dive deeper into how these systems reason and learn.

An LLM is akin to a sophisticated system designed to process input data and generate an output. For language models, the task is often autoregressive, meaning to predict the next piece of text (a word or word fragment, broken into machine understandable bits called ‘tokens’) given the preceding sequence of text, like an extremely advanced autocomplete. The model is composed of interconnected layers of mathematical operations, often visualized as a ‘neural network’. Crucially, these operations involve millions or even billions of parameters – numerical values that are adjusted during training. These parameters represent the ‘knowledge’ the model learns from the data. We mention training on errors but how are errors found? The output you usually see from LLM’s is a block of text or words but this output isn’t just text; it’s typically a probability distribution over the model’s entire vocabulary (all the unique words or tokens it knows). During training, we know the correct next word in the sequence from the training data. We need a way to measure how ‘wrong’ the model’s prediction is. This measure is called the loss function. The value computed by the loss function guides the learning process; the entire goal of training is to adjust the model’s parameters to minimize this loss value over the entire training dataset.
To illustrate, In a situation where the input tokens are ‘What is Einstein’s First name?’ loss could be 1 minus the probability the model assigned to “Albert”. If the model assigned 0.9 probability, the loss is 0.1; if 0.8, the loss is 0.2. This is similar to an ‘L1 loss’. However, in practice, language models (and many other deep learning systems) are typically trained using a different loss function: cross-entropy.
The theoretical motivation for cross-entropy relates to information theory (measuring the difference between probability distributions), but its calculation is straightforward. For a given prediction, we take the negative natural logarithm (-ln) of the probability the model assigned to the correct token.

- If the model is 100% confident (probability = 1), loss = -ln(1) = 0.
- If the model is 90% confident (probability = 0.9), loss = -ln(0.9) ≈ 0.105.
- If the model is only 10% confident (probability = 0.1), loss = -ln(0.1) ≈ 2.3.
- If the model is very unconfident (probability approaches 0), the loss shoots towards infinity.

Plotting cross-entropy loss shows it penalizes low-confidence correct predictions much more heavily than L1 loss. This property often leads to better training dynamics and model performance. The performance values plotted on the y-axis in scaling law graphs are typically this cross-entropy loss, averaged over a large set of unseen ‘test’ examples. Lower average cross-entropy means the model is, on average, more confident about the correct next tokens.
But wait, don’t we want no loss? Why can’t a Large Language Model reliably have a correct answer? The reason according to OpenAI is that predicting the next element in many real-world sequences doesn’t have a single, unambiguously correct answer. While “Einstein’s first name is” strongly implies “Albert”, consider a sequence like “A neural network is a”. Searching the internet reveals many valid continuations: “…model inspired by the brain,” “…type of machine learning algorithm,” “…powerful tool for pattern recognition,” etc. None are inherently wrong.
This concept is called entropy, the inherent uncertainty. Because of this fundamental entropy, the average cross-entropy loss can never truly reach zero, even with a perfect model. There will always be an irreducible error corresponding to the inherent unpredictability of the data itself. OpenAI studied this in depth and could even estimate this irreducible error by fitting scaling law equations that included a constant offset term (L = L₀ + A*N⁻ᵅ). Interestingly, their estimates for this constant term agreed whether they derived it from scaling with model size or scaling with compute.
All of this work just to end up at the same question. Why does AI model performance follow such simple laws? Why are data, model size, and compute the key limiting factors, connected to performance in this power-law fashion? While deep learning theory often lags behind practice, recent work offers a compelling geometric explanation: the data manifold hypothesis.

The data manifold hypothesis essentially states that real data lives in a special invisible area, even though your data technically exists in a huge space, all the meaningful examples cluster together along a much simpler path or surface. This area has fewer true dimensions than the input, with a neural network we are essentially mapping this area that exists through dimensions, connecting everything. By seeing the connections in input data and output we see that distance along this area actually has significance, we can see how data connects beyond the limits of the pieces of data individually. This explains why AI models can generalize from training examples to new examples they’ve never seen before – they’ve learned the underlying structure of the data, not just memorized individual examples.
This implies that we can find repeated patterns and relationships from this geometry, the more data (D) we have, the denser these points are. The average distance ‘s’ between neighboring training points depends on the manifold’s intrinsic dimension ‘d’. In 1D, s ∝ D⁻¹; in 2D, s ∝ D⁻¹/²; in 3D, s ∝ D⁻¹/³; generally, s ∝ D⁻¹/ᵈ. More data lets the model “resolve” the manifold’s shape more finely. With new test points, its prediction error is related to how far that point is from the nearest training point(s) on the learned manifold. Assuming the manifold is relatively smooth, and the model interpolates somewhat linearly between training points, theoretical analysis suggests the prediction error scales approximately with the square of the distance ‘s’. So, Error ∝ s² ∝ (D⁻¹/ᵈ)² = D⁻²/ᵈ.
Though scaling laws accurately predict average model performance improvements, they miss a critical phenomenon: the sudden emergence of entirely new capabilities (like complex reasoning or arithmetic) at specific scale thresholds rather than gradual improvement. Researchers still cannot reliably predict when or which abilities will emerge. Meanwhile, the field is advancing beyond simple scaling by exploring data pruning techniques that intelligently select higher-quality training examples and by investigating how minimal additional compute with specialized training objectives can deliver outsized performance gains. It’s also crucial to understand that the fundamental limits described by scaling laws relate to irreducible error inherent to tasks themselves, distinct from overfitting issues where models fail to generalize beyond their training data.

As we reach the frontier of our exploration, it becomes clear that while scaling laws give us a roadmap for AI advancement, they also hint at eventual horizons. The manifold hypothesis offers a compelling framework to understand why simply adding more data, parameters, and compute yields such predictable improvements, yet also suggests fundamental limits based on the intrinsic dimensionality of the problems we’re solving.
The future of AI likely won’t be defined by brute-force scaling alone. Instead, we’re entering an era where the quality of data, the sophistication of training objectives, and the nuanced understanding of emergent capabilities will drive progress. Much like how the efficient compute frontier represents a boundary for a given computational budget, there exists a theoretical performance ceiling governed by the irreducible entropy of our tasks.
However, the sudden emergence of new capabilities at various thresholds reminds us that AI development isn’t simply following a smooth curve, it contains surprising phase transitions we’re still learning to predict and understand. As we continue pushing these boundaries, the question transforms from “how soon until we reach the limit?” to “what creative approaches will help us transcend the limitations we currently perceive?”
